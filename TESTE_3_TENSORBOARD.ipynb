{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3517ca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-f7f55c70618a4125\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-f7f55c70618a4125\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "422/422 [==============================] - 17s 39ms/step - loss: 0.6868 - accuracy: 0.7559 - val_loss: 0.4374 - val_accuracy: 0.8445\n",
      "Epoch 2/5\n",
      "422/422 [==============================] - 16s 39ms/step - loss: 0.4480 - accuracy: 0.8406 - val_loss: 0.3856 - val_accuracy: 0.8605\n",
      "Epoch 3/5\n",
      "422/422 [==============================] - 19s 45ms/step - loss: 0.3994 - accuracy: 0.8574 - val_loss: 0.3510 - val_accuracy: 0.8765\n",
      "Epoch 4/5\n",
      "422/422 [==============================] - 20s 47ms/step - loss: 0.3731 - accuracy: 0.8671 - val_loss: 0.3357 - val_accuracy: 0.8798\n",
      "Epoch 5/5\n",
      "422/422 [==============================] - 20s 47ms/step - loss: 0.3517 - accuracy: 0.8732 - val_loss: 0.3174 - val_accuracy: 0.8847\n",
      "Parâmetros: Épocas=5, Tamanho do Lote=128, Função de Ativação=relu, Otimizador=adam\n",
      "Erro no teste: 0.33352\n",
      "Acurácia no teste: 0.882\n",
      "--------------------------------------------\n",
      "Epoch 1/5\n",
      "422/422 [==============================] - 21s 48ms/step - loss: 1.7096 - accuracy: 0.3958 - val_loss: 0.9573 - val_accuracy: 0.6797\n",
      "Epoch 2/5\n",
      "422/422 [==============================] - 22s 51ms/step - loss: 0.9484 - accuracy: 0.6533 - val_loss: 0.7328 - val_accuracy: 0.7312\n",
      "Epoch 3/5\n",
      "422/422 [==============================] - 24s 56ms/step - loss: 0.7861 - accuracy: 0.7119 - val_loss: 0.6472 - val_accuracy: 0.7615\n",
      "Epoch 4/5\n",
      "422/422 [==============================] - 25s 60ms/step - loss: 0.7062 - accuracy: 0.7409 - val_loss: 0.6064 - val_accuracy: 0.7695\n",
      "Epoch 5/5\n",
      "422/422 [==============================] - 22s 52ms/step - loss: 0.6569 - accuracy: 0.7579 - val_loss: 0.5638 - val_accuracy: 0.7952\n",
      "Parâmetros: Épocas=5, Tamanho do Lote=128, Função de Ativação=relu, Otimizador=sgd\n",
      "Erro no teste: 0.59317\n",
      "Acurácia no teste: 0.7827\n",
      "--------------------------------------------\n",
      "Epoch 1/5\n",
      "422/422 [==============================] - 20s 45ms/step - loss: 0.6673 - accuracy: 0.7596 - val_loss: 0.4613 - val_accuracy: 0.8345\n",
      "Epoch 2/5\n",
      "422/422 [==============================] - 21s 50ms/step - loss: 0.4522 - accuracy: 0.8380 - val_loss: 0.3853 - val_accuracy: 0.8627\n",
      "Epoch 3/5\n",
      "422/422 [==============================] - 25s 59ms/step - loss: 0.4014 - accuracy: 0.8579 - val_loss: 0.3521 - val_accuracy: 0.8738\n",
      "Epoch 4/5\n",
      "422/422 [==============================] - 29s 68ms/step - loss: 0.3716 - accuracy: 0.8664 - val_loss: 0.3323 - val_accuracy: 0.8805\n",
      "Epoch 5/5\n",
      "422/422 [==============================] - 27s 65ms/step - loss: 0.3518 - accuracy: 0.8753 - val_loss: 0.3149 - val_accuracy: 0.8843\n",
      "Parâmetros: Épocas=5, Tamanho do Lote=128, Função de Ativação=relu, Otimizador=rmsprop\n",
      "Erro no teste: 0.33201\n",
      "Acurácia no teste: 0.8821\n",
      "--------------------------------------------\n",
      "Epoch 1/5\n",
      "422/422 [==============================] - 31s 73ms/step - loss: 1.4229 - accuracy: 0.4813 - val_loss: 0.6918 - val_accuracy: 0.7507\n",
      "Epoch 2/5\n",
      "422/422 [==============================] - 27s 65ms/step - loss: 0.6970 - accuracy: 0.7396 - val_loss: 0.5784 - val_accuracy: 0.7830\n",
      "Epoch 3/5\n",
      "422/422 [==============================] - 32s 76ms/step - loss: 0.6180 - accuracy: 0.7685 - val_loss: 0.5328 - val_accuracy: 0.8047\n",
      "Epoch 4/5\n",
      "422/422 [==============================] - 32s 75ms/step - loss: 0.5742 - accuracy: 0.7851 - val_loss: 0.4892 - val_accuracy: 0.8148\n",
      "Epoch 5/5\n",
      "422/422 [==============================] - 32s 75ms/step - loss: 0.5432 - accuracy: 0.7977 - val_loss: 0.4698 - val_accuracy: 0.8283\n",
      "Parâmetros: Épocas=5, Tamanho do Lote=128, Função de Ativação=sigmoid, Otimizador=adam\n",
      "Erro no teste: 0.49825\n",
      "Acurácia no teste: 0.8177\n",
      "--------------------------------------------\n",
      "Epoch 1/5\n",
      "422/422 [==============================] - 31s 71ms/step - loss: 2.4523 - accuracy: 0.1017 - val_loss: 2.2959 - val_accuracy: 0.1910\n",
      "Epoch 2/5\n",
      "422/422 [==============================] - 35s 83ms/step - loss: 2.3555 - accuracy: 0.1068 - val_loss: 2.2889 - val_accuracy: 0.0973\n",
      "Epoch 3/5\n",
      "422/422 [==============================] - 30s 72ms/step - loss: 2.3197 - accuracy: 0.1132 - val_loss: 2.2746 - val_accuracy: 0.1227\n",
      "Epoch 4/5\n",
      "422/422 [==============================] - 30s 71ms/step - loss: 2.2847 - accuracy: 0.1329 - val_loss: 2.2334 - val_accuracy: 0.3330\n",
      "Epoch 5/5\n",
      "422/422 [==============================] - 31s 72ms/step - loss: 2.1992 - accuracy: 0.2026 - val_loss: 2.0617 - val_accuracy: 0.4927\n",
      "Parâmetros: Épocas=5, Tamanho do Lote=128, Função de Ativação=sigmoid, Otimizador=sgd\n",
      "Erro no teste: 2.06176\n",
      "Acurácia no teste: 0.4933\n",
      "--------------------------------------------\n",
      "Epoch 1/5\n",
      "422/422 [==============================] - 40s 92ms/step - loss: 1.3652 - accuracy: 0.5037 - val_loss: 0.6935 - val_accuracy: 0.7500\n",
      "Epoch 2/5\n",
      "422/422 [==============================] - 48s 114ms/step - loss: 0.7162 - accuracy: 0.7320 - val_loss: 0.5872 - val_accuracy: 0.7708\n",
      "Epoch 3/5\n",
      "422/422 [==============================] - 47s 110ms/step - loss: 0.6300 - accuracy: 0.7624 - val_loss: 0.5464 - val_accuracy: 0.7938\n",
      "Epoch 4/5\n",
      "422/422 [==============================] - 49s 116ms/step - loss: 0.5745 - accuracy: 0.7846 - val_loss: 0.4904 - val_accuracy: 0.8127\n",
      "Epoch 5/5\n",
      "422/422 [==============================] - 48s 114ms/step - loss: 0.5398 - accuracy: 0.7984 - val_loss: 0.4737 - val_accuracy: 0.8208\n",
      "Parâmetros: Épocas=5, Tamanho do Lote=128, Função de Ativação=sigmoid, Otimizador=rmsprop\n",
      "Erro no teste: 0.50616\n",
      "Acurácia no teste: 0.8094\n",
      "--------------------------------------------\n",
      "Epoch 1/5\n",
      "422/422 [==============================] - 30s 63ms/step - loss: 0.6610 - accuracy: 0.7631 - val_loss: 0.4441 - val_accuracy: 0.8413\n",
      "Epoch 2/5\n",
      "422/422 [==============================] - 29s 68ms/step - loss: 0.4535 - accuracy: 0.8378 - val_loss: 0.3924 - val_accuracy: 0.8563\n",
      "Epoch 3/5\n",
      "422/422 [==============================] - 40s 95ms/step - loss: 0.4121 - accuracy: 0.8537 - val_loss: 0.3578 - val_accuracy: 0.8715\n",
      "Epoch 4/5\n",
      "422/422 [==============================] - 44s 103ms/step - loss: 0.3840 - accuracy: 0.8636 - val_loss: 0.3441 - val_accuracy: 0.8758\n",
      "Epoch 5/5\n",
      "183/422 [============>.................] - ETA: 24s - loss: 0.3665 - accuracy: 0.8674"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# Carregar o conjunto de dados Fashion MNIST\n",
    "dataset = tf.keras.datasets.fashion_mnist\n",
    "(X_train, y_train), (X_test, y_test) = dataset.load_data()\n",
    "\n",
    "# Pré-processamento dos dados\n",
    "X_train = X_train.astype('float32') / 255\n",
    "X_test = X_test.astype('float32') / 255\n",
    "X_train = np.expand_dims(X_train, -1)\n",
    "X_test = np.expand_dims(X_test, -1)\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
    "\n",
    "# Definir as listas de parâmetros desejados\n",
    "batch_sizes = [128, 256, 512]\n",
    "num_epochs = [5, 10, 15]\n",
    "activation_functions = ['relu', 'sigmoid', 'tanh']\n",
    "optimizers = ['adam', 'sgd', 'rmsprop']\n",
    "\n",
    "# Criar o diretório de logs para o TensorBoard\n",
    "logdir = \"logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# Inicializar o TensorBoard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {logdir}\n",
    "\n",
    "# Loop para treinar e avaliar o modelo com várias combinações de parâmetros\n",
    "for batch_size in batch_sizes:\n",
    "    for num_epochs in num_epochs:\n",
    "        for activation_func in activation_functions:\n",
    "            for optimizer in optimizers:\n",
    "                # Definir o modelo\n",
    "                num_classes = 10\n",
    "                input_shape = (28, 28, 1)\n",
    "                model = tf.keras.Sequential([\n",
    "                    tf.keras.Input(shape=input_shape),\n",
    "                    tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation=activation_func),\n",
    "                    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "                    tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation=activation_func),\n",
    "                    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "                    tf.keras.layers.Flatten(),\n",
    "                    tf.keras.layers.Dropout(0.5),\n",
    "                    tf.keras.layers.Dense(num_classes, activation=\"softmax\")\n",
    "                ])\n",
    "\n",
    "                # Compilar o modelo\n",
    "                model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "                # Criar o callback do TensorBoard\n",
    "                tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "                # Treinar o modelo com o callback do TensorBoard\n",
    "                model.fit(X_train, y_train, batch_size=batch_size, epochs=num_epochs,\n",
    "                          validation_split=0.1, callbacks=[tensorboard_callback])\n",
    "\n",
    "                # Avaliar o modelo nos dados de teste\n",
    "                score = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "                # Imprimir os resultados\n",
    "                print(f\"Parâmetros: Épocas={num_epochs}, Tamanho do Lote={batch_size}, \"\n",
    "                      f\"Função de Ativação={activation_func}, Otimizador={optimizer}\")\n",
    "                print(f\"Erro no teste: {round(score[0], 5)}\")\n",
    "                print(f\"Acurácia no teste: {round(score[1], 5)}\")\n",
    "                print(\"--------------------------------------------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7c5eec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
